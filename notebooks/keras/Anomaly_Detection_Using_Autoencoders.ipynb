{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Anomaly-Detection-Using-Autoencoders.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ykato27/Anomaly-Detection/blob/main/Anomaly_Detection_Using_Autoencoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7Np6RCC8nbAJ"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YL7Kir1Jvwzc"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "ATDPsAdxnsr6",
    "outputId": "d59c0503-d1c6-4d97-e649-74ead0450d3b"
   },
   "source": [
    "# Download the dataset\n",
    "PATH_TO_DATA = \"http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv\"\n",
    "data = pd.read_csv(PATH_TO_DATA, header=None)\n",
    "print(data.shape)\n",
    "data.head()"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(4998, 141)\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.112522</td>\n",
       "      <td>-2.827204</td>\n",
       "      <td>-3.773897</td>\n",
       "      <td>-4.349751</td>\n",
       "      <td>-4.376041</td>\n",
       "      <td>-3.474986</td>\n",
       "      <td>-2.181408</td>\n",
       "      <td>-1.818287</td>\n",
       "      <td>-1.250522</td>\n",
       "      <td>-0.477492</td>\n",
       "      <td>-0.363808</td>\n",
       "      <td>-0.491957</td>\n",
       "      <td>-0.421855</td>\n",
       "      <td>-0.309201</td>\n",
       "      <td>-0.495939</td>\n",
       "      <td>-0.342119</td>\n",
       "      <td>-0.355336</td>\n",
       "      <td>-0.367913</td>\n",
       "      <td>-0.316503</td>\n",
       "      <td>-0.412374</td>\n",
       "      <td>-0.471672</td>\n",
       "      <td>-0.413458</td>\n",
       "      <td>-0.364617</td>\n",
       "      <td>-0.449298</td>\n",
       "      <td>-0.471419</td>\n",
       "      <td>-0.424777</td>\n",
       "      <td>-0.462517</td>\n",
       "      <td>-0.552472</td>\n",
       "      <td>-0.475375</td>\n",
       "      <td>-0.694200</td>\n",
       "      <td>-0.701868</td>\n",
       "      <td>-0.593812</td>\n",
       "      <td>-0.660684</td>\n",
       "      <td>-0.713831</td>\n",
       "      <td>-0.769807</td>\n",
       "      <td>-0.672282</td>\n",
       "      <td>-0.653676</td>\n",
       "      <td>-0.639406</td>\n",
       "      <td>-0.559302</td>\n",
       "      <td>-0.591670</td>\n",
       "      <td>...</td>\n",
       "      <td>1.258179</td>\n",
       "      <td>1.433789</td>\n",
       "      <td>1.700533</td>\n",
       "      <td>1.999043</td>\n",
       "      <td>2.125341</td>\n",
       "      <td>1.993291</td>\n",
       "      <td>1.932246</td>\n",
       "      <td>1.797437</td>\n",
       "      <td>1.522284</td>\n",
       "      <td>1.251168</td>\n",
       "      <td>0.998730</td>\n",
       "      <td>0.483722</td>\n",
       "      <td>0.023132</td>\n",
       "      <td>-0.194914</td>\n",
       "      <td>-0.220917</td>\n",
       "      <td>-0.243737</td>\n",
       "      <td>-0.254695</td>\n",
       "      <td>-0.291136</td>\n",
       "      <td>-0.256490</td>\n",
       "      <td>-0.227874</td>\n",
       "      <td>-0.322423</td>\n",
       "      <td>-0.289286</td>\n",
       "      <td>-0.318170</td>\n",
       "      <td>-0.363654</td>\n",
       "      <td>-0.393456</td>\n",
       "      <td>-0.266419</td>\n",
       "      <td>-0.256823</td>\n",
       "      <td>-0.288694</td>\n",
       "      <td>-0.162338</td>\n",
       "      <td>0.160348</td>\n",
       "      <td>0.792168</td>\n",
       "      <td>0.933541</td>\n",
       "      <td>0.796958</td>\n",
       "      <td>0.578621</td>\n",
       "      <td>0.257740</td>\n",
       "      <td>0.228077</td>\n",
       "      <td>0.123431</td>\n",
       "      <td>0.925286</td>\n",
       "      <td>0.193137</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.100878</td>\n",
       "      <td>-3.996840</td>\n",
       "      <td>-4.285843</td>\n",
       "      <td>-4.506579</td>\n",
       "      <td>-4.022377</td>\n",
       "      <td>-3.234368</td>\n",
       "      <td>-1.566126</td>\n",
       "      <td>-0.992258</td>\n",
       "      <td>-0.754680</td>\n",
       "      <td>0.042321</td>\n",
       "      <td>0.148951</td>\n",
       "      <td>0.183527</td>\n",
       "      <td>0.294876</td>\n",
       "      <td>0.190233</td>\n",
       "      <td>0.235575</td>\n",
       "      <td>0.253487</td>\n",
       "      <td>0.221742</td>\n",
       "      <td>0.050233</td>\n",
       "      <td>0.178042</td>\n",
       "      <td>0.139563</td>\n",
       "      <td>0.046794</td>\n",
       "      <td>0.043007</td>\n",
       "      <td>0.106544</td>\n",
       "      <td>0.012654</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.045724</td>\n",
       "      <td>-0.045999</td>\n",
       "      <td>-0.072667</td>\n",
       "      <td>-0.071078</td>\n",
       "      <td>-0.153866</td>\n",
       "      <td>-0.227254</td>\n",
       "      <td>-0.249270</td>\n",
       "      <td>-0.253489</td>\n",
       "      <td>-0.332835</td>\n",
       "      <td>-0.264330</td>\n",
       "      <td>-0.345825</td>\n",
       "      <td>-0.310781</td>\n",
       "      <td>-0.334160</td>\n",
       "      <td>-0.306178</td>\n",
       "      <td>-0.174563</td>\n",
       "      <td>...</td>\n",
       "      <td>1.808428</td>\n",
       "      <td>2.164346</td>\n",
       "      <td>2.070747</td>\n",
       "      <td>1.903614</td>\n",
       "      <td>1.764455</td>\n",
       "      <td>1.507769</td>\n",
       "      <td>1.293428</td>\n",
       "      <td>0.894562</td>\n",
       "      <td>0.578016</td>\n",
       "      <td>0.244343</td>\n",
       "      <td>-0.286443</td>\n",
       "      <td>-0.515881</td>\n",
       "      <td>-0.732707</td>\n",
       "      <td>-0.832465</td>\n",
       "      <td>-0.803318</td>\n",
       "      <td>-0.836252</td>\n",
       "      <td>-0.777865</td>\n",
       "      <td>-0.774753</td>\n",
       "      <td>-0.733404</td>\n",
       "      <td>-0.721386</td>\n",
       "      <td>-0.832095</td>\n",
       "      <td>-0.711982</td>\n",
       "      <td>-0.751867</td>\n",
       "      <td>-0.757720</td>\n",
       "      <td>-0.853120</td>\n",
       "      <td>-0.766988</td>\n",
       "      <td>-0.688161</td>\n",
       "      <td>-0.519923</td>\n",
       "      <td>0.039406</td>\n",
       "      <td>0.560327</td>\n",
       "      <td>0.538356</td>\n",
       "      <td>0.656881</td>\n",
       "      <td>0.787490</td>\n",
       "      <td>0.724046</td>\n",
       "      <td>0.555784</td>\n",
       "      <td>0.476333</td>\n",
       "      <td>0.773820</td>\n",
       "      <td>1.119621</td>\n",
       "      <td>-1.436250</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.567088</td>\n",
       "      <td>-2.593450</td>\n",
       "      <td>-3.874230</td>\n",
       "      <td>-4.584095</td>\n",
       "      <td>-4.187449</td>\n",
       "      <td>-3.151462</td>\n",
       "      <td>-1.742940</td>\n",
       "      <td>-1.490658</td>\n",
       "      <td>-1.183580</td>\n",
       "      <td>-0.394229</td>\n",
       "      <td>-0.282897</td>\n",
       "      <td>-0.356926</td>\n",
       "      <td>-0.287297</td>\n",
       "      <td>-0.399489</td>\n",
       "      <td>-0.473244</td>\n",
       "      <td>-0.379048</td>\n",
       "      <td>-0.399039</td>\n",
       "      <td>-0.178594</td>\n",
       "      <td>-0.339522</td>\n",
       "      <td>-0.498447</td>\n",
       "      <td>-0.337251</td>\n",
       "      <td>-0.425480</td>\n",
       "      <td>-0.423952</td>\n",
       "      <td>-0.463170</td>\n",
       "      <td>-0.493253</td>\n",
       "      <td>-0.549749</td>\n",
       "      <td>-0.529831</td>\n",
       "      <td>-0.530935</td>\n",
       "      <td>-0.502365</td>\n",
       "      <td>-0.417368</td>\n",
       "      <td>-0.526346</td>\n",
       "      <td>-0.471005</td>\n",
       "      <td>-0.676784</td>\n",
       "      <td>-0.898612</td>\n",
       "      <td>-0.610571</td>\n",
       "      <td>-0.530164</td>\n",
       "      <td>-0.765674</td>\n",
       "      <td>-0.581937</td>\n",
       "      <td>-0.537848</td>\n",
       "      <td>-0.556386</td>\n",
       "      <td>...</td>\n",
       "      <td>1.810988</td>\n",
       "      <td>2.185398</td>\n",
       "      <td>2.262985</td>\n",
       "      <td>2.052920</td>\n",
       "      <td>1.890488</td>\n",
       "      <td>1.793033</td>\n",
       "      <td>1.564784</td>\n",
       "      <td>1.234619</td>\n",
       "      <td>0.900302</td>\n",
       "      <td>0.551957</td>\n",
       "      <td>0.258222</td>\n",
       "      <td>-0.128587</td>\n",
       "      <td>-0.092585</td>\n",
       "      <td>-0.168606</td>\n",
       "      <td>-0.495989</td>\n",
       "      <td>-0.395034</td>\n",
       "      <td>-0.328238</td>\n",
       "      <td>-0.448138</td>\n",
       "      <td>-0.268230</td>\n",
       "      <td>-0.456415</td>\n",
       "      <td>-0.357867</td>\n",
       "      <td>-0.317508</td>\n",
       "      <td>-0.434112</td>\n",
       "      <td>-0.549203</td>\n",
       "      <td>-0.324615</td>\n",
       "      <td>-0.268082</td>\n",
       "      <td>-0.220384</td>\n",
       "      <td>-0.117429</td>\n",
       "      <td>0.614059</td>\n",
       "      <td>1.284825</td>\n",
       "      <td>0.886073</td>\n",
       "      <td>0.531452</td>\n",
       "      <td>0.311377</td>\n",
       "      <td>-0.021919</td>\n",
       "      <td>-0.713683</td>\n",
       "      <td>-0.532197</td>\n",
       "      <td>0.321097</td>\n",
       "      <td>0.904227</td>\n",
       "      <td>-0.421797</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.490473</td>\n",
       "      <td>-1.914407</td>\n",
       "      <td>-3.616364</td>\n",
       "      <td>-4.318823</td>\n",
       "      <td>-4.268016</td>\n",
       "      <td>-3.881110</td>\n",
       "      <td>-2.993280</td>\n",
       "      <td>-1.671131</td>\n",
       "      <td>-1.333884</td>\n",
       "      <td>-0.965629</td>\n",
       "      <td>-0.183319</td>\n",
       "      <td>-0.101657</td>\n",
       "      <td>-0.273874</td>\n",
       "      <td>-0.127818</td>\n",
       "      <td>-0.195983</td>\n",
       "      <td>-0.213523</td>\n",
       "      <td>-0.176473</td>\n",
       "      <td>-0.156932</td>\n",
       "      <td>-0.149172</td>\n",
       "      <td>-0.181510</td>\n",
       "      <td>-0.180074</td>\n",
       "      <td>-0.246151</td>\n",
       "      <td>-0.274260</td>\n",
       "      <td>-0.140960</td>\n",
       "      <td>-0.277449</td>\n",
       "      <td>-0.382549</td>\n",
       "      <td>-0.311937</td>\n",
       "      <td>-0.360093</td>\n",
       "      <td>-0.405968</td>\n",
       "      <td>-0.571433</td>\n",
       "      <td>-0.524106</td>\n",
       "      <td>-0.537886</td>\n",
       "      <td>-0.606778</td>\n",
       "      <td>-0.661446</td>\n",
       "      <td>-0.683375</td>\n",
       "      <td>-0.746683</td>\n",
       "      <td>-0.635662</td>\n",
       "      <td>-0.625231</td>\n",
       "      <td>-0.540094</td>\n",
       "      <td>-0.674995</td>\n",
       "      <td>...</td>\n",
       "      <td>1.772155</td>\n",
       "      <td>2.000769</td>\n",
       "      <td>1.925003</td>\n",
       "      <td>1.898426</td>\n",
       "      <td>1.720953</td>\n",
       "      <td>1.501711</td>\n",
       "      <td>1.422492</td>\n",
       "      <td>1.023225</td>\n",
       "      <td>0.776341</td>\n",
       "      <td>0.504426</td>\n",
       "      <td>0.056382</td>\n",
       "      <td>-0.233161</td>\n",
       "      <td>-0.406388</td>\n",
       "      <td>-0.327528</td>\n",
       "      <td>-0.460868</td>\n",
       "      <td>-0.402536</td>\n",
       "      <td>-0.345752</td>\n",
       "      <td>-0.354206</td>\n",
       "      <td>-0.439959</td>\n",
       "      <td>-0.425326</td>\n",
       "      <td>-0.439789</td>\n",
       "      <td>-0.451835</td>\n",
       "      <td>-0.395926</td>\n",
       "      <td>-0.448762</td>\n",
       "      <td>-0.391789</td>\n",
       "      <td>-0.376307</td>\n",
       "      <td>-0.461069</td>\n",
       "      <td>-0.253524</td>\n",
       "      <td>0.213006</td>\n",
       "      <td>0.491173</td>\n",
       "      <td>0.350816</td>\n",
       "      <td>0.499111</td>\n",
       "      <td>0.600345</td>\n",
       "      <td>0.842069</td>\n",
       "      <td>0.952074</td>\n",
       "      <td>0.990133</td>\n",
       "      <td>1.086798</td>\n",
       "      <td>1.403011</td>\n",
       "      <td>-0.383564</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.800232</td>\n",
       "      <td>-0.874252</td>\n",
       "      <td>-2.384761</td>\n",
       "      <td>-3.973292</td>\n",
       "      <td>-4.338224</td>\n",
       "      <td>-3.802422</td>\n",
       "      <td>-2.534510</td>\n",
       "      <td>-1.783423</td>\n",
       "      <td>-1.594450</td>\n",
       "      <td>-0.753199</td>\n",
       "      <td>-0.298107</td>\n",
       "      <td>-0.428928</td>\n",
       "      <td>-0.491351</td>\n",
       "      <td>-0.361304</td>\n",
       "      <td>-0.339296</td>\n",
       "      <td>-0.324952</td>\n",
       "      <td>-0.290113</td>\n",
       "      <td>-0.363051</td>\n",
       "      <td>-0.525684</td>\n",
       "      <td>-0.597423</td>\n",
       "      <td>-0.575523</td>\n",
       "      <td>-0.567503</td>\n",
       "      <td>-0.504555</td>\n",
       "      <td>-0.618406</td>\n",
       "      <td>-0.682814</td>\n",
       "      <td>-0.743849</td>\n",
       "      <td>-0.815588</td>\n",
       "      <td>-0.826902</td>\n",
       "      <td>-0.782374</td>\n",
       "      <td>-0.929462</td>\n",
       "      <td>-0.999672</td>\n",
       "      <td>-1.060969</td>\n",
       "      <td>-1.007877</td>\n",
       "      <td>-1.028735</td>\n",
       "      <td>-1.122629</td>\n",
       "      <td>-1.028650</td>\n",
       "      <td>-1.046515</td>\n",
       "      <td>-1.063372</td>\n",
       "      <td>-1.122423</td>\n",
       "      <td>-0.983242</td>\n",
       "      <td>...</td>\n",
       "      <td>1.155363</td>\n",
       "      <td>1.336254</td>\n",
       "      <td>1.627534</td>\n",
       "      <td>1.717594</td>\n",
       "      <td>1.696487</td>\n",
       "      <td>1.741686</td>\n",
       "      <td>1.674078</td>\n",
       "      <td>1.546928</td>\n",
       "      <td>1.331738</td>\n",
       "      <td>1.110168</td>\n",
       "      <td>0.922210</td>\n",
       "      <td>0.521777</td>\n",
       "      <td>0.154852</td>\n",
       "      <td>-0.123861</td>\n",
       "      <td>-0.202998</td>\n",
       "      <td>-0.247956</td>\n",
       "      <td>-0.219122</td>\n",
       "      <td>-0.214695</td>\n",
       "      <td>-0.319215</td>\n",
       "      <td>-0.198597</td>\n",
       "      <td>-0.151618</td>\n",
       "      <td>-0.129593</td>\n",
       "      <td>-0.074939</td>\n",
       "      <td>-0.196807</td>\n",
       "      <td>-0.174795</td>\n",
       "      <td>-0.208833</td>\n",
       "      <td>-0.210754</td>\n",
       "      <td>-0.100485</td>\n",
       "      <td>0.197446</td>\n",
       "      <td>0.966606</td>\n",
       "      <td>1.148884</td>\n",
       "      <td>0.958434</td>\n",
       "      <td>1.059025</td>\n",
       "      <td>1.371682</td>\n",
       "      <td>1.277392</td>\n",
       "      <td>0.960304</td>\n",
       "      <td>0.971020</td>\n",
       "      <td>1.614392</td>\n",
       "      <td>1.421456</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3    ...       137       138       139  140\n",
       "0 -0.112522 -2.827204 -3.773897 -4.349751  ...  0.123431  0.925286  0.193137  1.0\n",
       "1 -1.100878 -3.996840 -4.285843 -4.506579  ...  0.773820  1.119621 -1.436250  1.0\n",
       "2 -0.567088 -2.593450 -3.874230 -4.584095  ...  0.321097  0.904227 -0.421797  1.0\n",
       "3  0.490473 -1.914407 -3.616364 -4.318823  ...  1.086798  1.403011 -0.383564  1.0\n",
       "4  0.800232 -0.874252 -2.384761 -3.973292  ...  0.971020  1.614392  1.421456  1.0\n",
       "\n",
       "[5 rows x 141 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV6KUkRgvoOK"
   },
   "source": [
    "## Split the data for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OTEDLRtA4I9k"
   },
   "source": [
    "# last column is the target\n",
    "# 0 = anomaly, 1 = normal\n",
    "TARGET = 140\n",
    "\n",
    "features = data.drop(TARGET, axis=1)\n",
    "target = data[TARGET]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    features, target, test_size=0.2, stratify=target\n",
    ")"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2mjzHNYjLviR"
   },
   "source": [
    "# use case is novelty detection so use only the normal data\n",
    "# for training\n",
    "train_index = y_train[y_train == 1].index\n",
    "train_data = x_train.loc[train_index]"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfk5mwbhv03v"
   },
   "source": [
    "## Scale the data using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cmJhTiuBts4F"
   },
   "source": [
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_train_scaled = min_max_scaler.fit_transform(train_data.copy())\n",
    "x_test_scaled = min_max_scaler.transform(x_test.copy())"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFLk9eJrwqvl"
   },
   "source": [
    "## Build an AutoEncoder model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N9VmLu89uPIY"
   },
   "source": [
    "# create a model by subclassing Model class in tensorflow\n",
    "class AutoEncoder(Model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    output_units: int\n",
    "      Number of output units\n",
    "\n",
    "    code_size: int\n",
    "      Number of units in bottle neck\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_units, code_size=8):\n",
    "        super().__init__()\n",
    "        self.encoder = Sequential(\n",
    "            [\n",
    "                Dense(64, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(16, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(code_size, activation=\"relu\"),\n",
    "            ]\n",
    "        )\n",
    "        self.decoder = Sequential(\n",
    "            [\n",
    "                Dense(16, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(32, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(64, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(output_units, activation=\"sigmoid\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-oylLrsQuPD5",
    "outputId": "207eae4b-1d39-484e-a0cd-75d1f540780a"
   },
   "source": [
    "model = AutoEncoder(output_units=x_train_scaled.shape[1])\n",
    "# configurations of model\n",
    "model.compile(loss=\"msle\", metrics=[\"mse\"], optimizer=\"adam\")\n",
    "\n",
    "history = model.fit(\n",
    "    x_train_scaled,\n",
    "    x_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_test_scaled, x_test_scaled),\n",
    ")"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 62ms/step - loss: 0.0114 - mse: 0.0256 - val_loss: 0.0132 - val_mse: 0.0304\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0108 - mse: 0.0242 - val_loss: 0.0128 - val_mse: 0.0294\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0098 - mse: 0.0218 - val_loss: 0.0125 - val_mse: 0.0287\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0087 - mse: 0.0194 - val_loss: 0.0121 - val_mse: 0.0278\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0076 - mse: 0.0170 - val_loss: 0.0117 - val_mse: 0.0270\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0067 - mse: 0.0149 - val_loss: 0.0115 - val_mse: 0.0264\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0061 - mse: 0.0136 - val_loss: 0.0110 - val_mse: 0.0253\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0057 - mse: 0.0126 - val_loss: 0.0106 - val_mse: 0.0245\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0054 - mse: 0.0120 - val_loss: 0.0103 - val_mse: 0.0238\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0051 - mse: 0.0114 - val_loss: 0.0100 - val_mse: 0.0232\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0050 - mse: 0.0111 - val_loss: 0.0098 - val_mse: 0.0227\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0049 - mse: 0.0109 - val_loss: 0.0097 - val_mse: 0.0225\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0048 - mse: 0.0107 - val_loss: 0.0096 - val_mse: 0.0223\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0047 - mse: 0.0106 - val_loss: 0.0096 - val_mse: 0.0223\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0047 - mse: 0.0105 - val_loss: 0.0096 - val_mse: 0.0222\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0046 - mse: 0.0104 - val_loss: 0.0096 - val_mse: 0.0222\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0046 - mse: 0.0103 - val_loss: 0.0096 - val_mse: 0.0223\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0046 - mse: 0.0102 - val_loss: 0.0095 - val_mse: 0.0221\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0045 - mse: 0.0102 - val_loss: 0.0095 - val_mse: 0.0220\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0045 - mse: 0.0101 - val_loss: 0.0094 - val_mse: 0.0220\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0045 - mse: 0.0100 - val_loss: 0.0094 - val_mse: 0.0219\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0044 - mse: 0.0099 - val_loss: 0.0094 - val_mse: 0.0218\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0044 - mse: 0.0099 - val_loss: 0.0094 - val_mse: 0.0218\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0043 - mse: 0.0097 - val_loss: 0.0093 - val_mse: 0.0217\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0043 - mse: 0.0097 - val_loss: 0.0093 - val_mse: 0.0216\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0043 - mse: 0.0096 - val_loss: 0.0093 - val_mse: 0.0215\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0042 - mse: 0.0095 - val_loss: 0.0092 - val_mse: 0.0214\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0042 - mse: 0.0094 - val_loss: 0.0092 - val_mse: 0.0213\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0041 - mse: 0.0093 - val_loss: 0.0091 - val_mse: 0.0212\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0041 - mse: 0.0092 - val_loss: 0.0091 - val_mse: 0.0212\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0040 - mse: 0.0090 - val_loss: 0.0091 - val_mse: 0.0212\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0040 - mse: 0.0089 - val_loss: 0.0091 - val_mse: 0.0211\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0039 - mse: 0.0088 - val_loss: 0.0091 - val_mse: 0.0210\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0038 - mse: 0.0086 - val_loss: 0.0090 - val_mse: 0.0210\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0038 - mse: 0.0085 - val_loss: 0.0090 - val_mse: 0.0209\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0037 - mse: 0.0084 - val_loss: 0.0090 - val_mse: 0.0209\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0037 - mse: 0.0083 - val_loss: 0.0090 - val_mse: 0.0208\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0036 - mse: 0.0081 - val_loss: 0.0090 - val_mse: 0.0209\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0036 - mse: 0.0081 - val_loss: 0.0090 - val_mse: 0.0208\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0035 - mse: 0.0080 - val_loss: 0.0090 - val_mse: 0.0208\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0035 - mse: 0.0079 - val_loss: 0.0089 - val_mse: 0.0206\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0035 - mse: 0.0078 - val_loss: 0.0089 - val_mse: 0.0206\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0034 - mse: 0.0077 - val_loss: 0.0088 - val_mse: 0.0205\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0034 - mse: 0.0077 - val_loss: 0.0088 - val_mse: 0.0204\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0034 - mse: 0.0076 - val_loss: 0.0088 - val_mse: 0.0203\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0034 - mse: 0.0075 - val_loss: 0.0087 - val_mse: 0.0201\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - mse: 0.0075 - val_loss: 0.0086 - val_mse: 0.0200\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0033 - mse: 0.0074 - val_loss: 0.0086 - val_mse: 0.0198\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0033 - mse: 0.0073 - val_loss: 0.0085 - val_mse: 0.0197\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0032 - mse: 0.0073 - val_loss: 0.0085 - val_mse: 0.0196\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCD6ykuG6wvV"
   },
   "source": [
    "## Plot history"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "ohEpArnj6NMG",
    "outputId": "aff2bd26-0e0c-4cee-ebff-5d59075271c3"
   },
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSLE Loss\")\n",
    "plt.legend([\"loss\", \"val_loss\"])\n",
    "plt.show()"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPNtOnsENWwc"
   },
   "source": [
    "## Find threshold"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ACFZcFWEFNpQ"
   },
   "source": [
    "def find_threshold(model, x_train_scaled):\n",
    "    reconstructions = model.predict(x_train_scaled)\n",
    "    # provides losses of individual instances\n",
    "    reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n",
    "\n",
    "    # threshold for anomaly scores\n",
    "    threshold = np.mean(reconstruction_errors.numpy()) + np.std(\n",
    "        reconstruction_errors.numpy()\n",
    "    )\n",
    "    return threshold\n",
    "\n",
    "\n",
    "def find_threshold_method_two(model, x_train_scaled):\n",
    "    # another method to find threshold\n",
    "    reconstructions = model.predict(x_train_scaled)\n",
    "    # provides losses of individual instances\n",
    "    reconstruction_errors = tf.keras.losses.msle(reconstructions, x_train_scaled)\n",
    "\n",
    "    threshold_2 = np.percentile(reconstruction_errors, 95)\n",
    "    return threshold_2\n",
    "\n",
    "\n",
    "def get_predictions(model, x_test_scaled, threshold):\n",
    "    predictions = model.predict(x_test_scaled)\n",
    "    # provides losses of individual instances\n",
    "    errors = tf.keras.losses.msle(predictions, x_test_scaled)\n",
    "    # 0 = anomaly, 1 = normal\n",
    "    anomaly_mask = pd.Series(errors) > threshold\n",
    "    preds = anomaly_mask.map(lambda x: 0.0 if x == True else 1.0)\n",
    "    return preds"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4Mxr7cnHPk-",
    "outputId": "12c7b918-6aca-4f66-ab2d-adda7f94aada"
   },
   "source": [
    "threshold = find_threshold(model, x_train_scaled)\n",
    "print(f\"Threshold method one: {threshold}\")\n",
    "\n",
    "threshold_2 = find_threshold_method_two(model, x_train_scaled)\n",
    "print(f\"Threshold method two: {threshold_2}\")"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Threshold method one: 0.007352647996364852\n",
      "Threshold method two: 0.010323284471302695\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bl0Op0GNHVDD",
    "outputId": "352f4e48-80af-4b14-fb4b-67e040f1af7b"
   },
   "source": [
    "preds = get_predictions(model, x_test_scaled, threshold)\n",
    "accuracy_score(preds, y_test)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.959"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TSdu3Uk7ASm"
   },
   "source": [
    "## Tuning AutoEncoder using keras tuner\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Zt5CaPNCh__",
    "outputId": "4ef7b54a-fa97-4644-bb37-2995d1a95922"
   },
   "source": [
    "!pip install -U keras-tuner"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting keras-tuner\n",
      "  Downloading keras_tuner-1.0.3-py3-none-any.whl (96 kB)\n",
      "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 22.7 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40 kB 33.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61 kB 24.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81 kB 22.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 92 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 96 kB 5.2 MB/s \n",
      "\u001b[?25hCollecting kt-legacy\n",
      "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.0.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.39.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.5)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.34.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.6.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.5.0)\n",
      "Installing collected packages: kt-legacy, keras-tuner\n",
      "Successfully installed keras-tuner-1.0.3 kt-legacy-1.0.4\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-AwQid85DbyC",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fc0b9b42-5a6d-4af2-8cf5-cfa5e2194ed0"
   },
   "source": [
    "import kerastuner as kt\n",
    "\n",
    "\n",
    "class AutoEncoderTuner(Model):\n",
    "    def __init__(self, hp, output_units, code_size=8):\n",
    "        super().__init__()\n",
    "        dense_1_units = hp.Int(\"dense_1_units\", min_value=16, max_value=72, step=4)\n",
    "        dense_2_units = hp.Int(\"dense_2_units\", min_value=16, max_value=72, step=4)\n",
    "        dense_3_units = hp.Int(\"dense_3_units\", min_value=16, max_value=72, step=4)\n",
    "        dense_4_units = hp.Int(\"dense_4_units\", min_value=16, max_value=72, step=4)\n",
    "        dense_5_units = hp.Int(\"dense_5_units\", min_value=16, max_value=72, step=4)\n",
    "        dense_6_units = hp.Int(\"dense_6_units\", min_value=16, max_value=72, step=4)\n",
    "\n",
    "        self.encoder = Sequential(\n",
    "            [\n",
    "                Dense(dense_1_units, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(dense_2_units, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(dense_3_units, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(code_size, activation=\"relu\"),\n",
    "            ]\n",
    "        )\n",
    "        self.decoder = Sequential(\n",
    "            [\n",
    "                Dense(dense_4_units, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(dense_5_units, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(dense_6_units, activation=\"relu\"),\n",
    "                Dropout(0.1),\n",
    "                Dense(output_units, activation=\"sigmoid\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    model = AutoEncoderTuner(hp, 140)\n",
    "    hp_learning_rate = hp.Choice(\"learning_rate\", values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(\n",
    "        loss=\"msle\",\n",
    "        optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "    )\n",
    "    return model"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6X0KEcX6NAS",
    "outputId": "7bb835e8-0c59-49a9-979c-594b3f96c80b"
   },
   "source": [
    "tuner = kt.Hyperband(\n",
    "    build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_epochs=20,\n",
    "    factor=3,\n",
    "    directory=\"autoencoder\",\n",
    "    project_name=\"tuning_autoencoder6\",\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    x_train_scaled,\n",
    "    x_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_test_scaled, x_test_scaled),\n",
    ")"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 02s]\n",
      "val_loss: 0.008567025884985924\n",
      "\n",
      "Best val_loss So Far: 0.008567025884985924\n",
      "Total elapsed time: 00h 01m 01s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tiB8zhcF0QV",
    "outputId": "0e6ef30f-f65a-4787-afe1-b2ca5ab27163"
   },
   "source": [
    "hparams = [f\"dense_{i}_units\" for i in range(1, 7)] + [\"learning_rate\"]\n",
    "best_hyperparams = tuner.get_best_hyperparameters()\n",
    "for hps in hparams:\n",
    "    print(f\"{hps}: {best_hyperparams[0][hps]}\")"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "dense_1_units: 56\n",
      "dense_2_units: 48\n",
      "dense_3_units: 16\n",
      "dense_4_units: 36\n",
      "dense_5_units: 32\n",
      "dense_6_units: 68\n",
      "learning_rate: 0.01\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUvDIqvVEuNk",
    "outputId": "b16afbe0-7d9f-45db-e8af-4bf74b37ffa9"
   },
   "source": [
    "best_model = tuner.get_best_models()[0]\n",
    "best_model.compile(loss=\"msle\", optimizer=Adam(0.001))\n",
    "\n",
    "best_model.fit(\n",
    "    x_train_scaled,\n",
    "    x_train_scaled,\n",
    "    epochs=50,\n",
    "    batch_size=512,\n",
    "    validation_data=(x_test_scaled, x_test_scaled),\n",
    ")"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 [==============================] - 1s 48ms/step - loss: 0.0029 - val_loss: 0.0085\n",
      "Epoch 2/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0029 - val_loss: 0.0086\n",
      "Epoch 3/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0029 - val_loss: 0.0086\n",
      "Epoch 4/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0029 - val_loss: 0.0086\n",
      "Epoch 5/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0029 - val_loss: 0.0086\n",
      "Epoch 6/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 7/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 8/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 9/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 10/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 11/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 12/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 13/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 14/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 15/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 16/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 17/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 18/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 19/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0084\n",
      "Epoch 20/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0028 - val_loss: 0.0084\n",
      "Epoch 21/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0028 - val_loss: 0.0085\n",
      "Epoch 22/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0028 - val_loss: 0.0084\n",
      "Epoch 23/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0085\n",
      "Epoch 24/50\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.0027 - val_loss: 0.0085\n",
      "Epoch 25/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0085\n",
      "Epoch 26/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 27/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 28/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 29/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 30/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 31/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 32/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 33/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 34/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 35/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 36/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 37/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 38/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0027 - val_loss: 0.0084\n",
      "Epoch 39/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - val_loss: 0.0083\n",
      "Epoch 40/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0027 - val_loss: 0.0083\n",
      "Epoch 41/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - val_loss: 0.0084\n",
      "Epoch 42/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0026 - val_loss: 0.0084\n",
      "Epoch 43/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 44/50\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 45/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 46/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 47/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 48/50\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 49/50\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.0026 - val_loss: 0.0083\n",
      "Epoch 50/50\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.0026 - val_loss: 0.0083\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6578ad36d0>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H0DvGEOkHdsn",
    "outputId": "c1c0eb69-3c08-4241-d4c6-f9fcaac1972d"
   },
   "source": [
    "threshold_ = find_threshold(best_model, x_train_scaled)\n",
    "preds_ = get_predictions(best_model, x_test_scaled, threshold_)\n",
    "accuracy_score(preds_, y_test)"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.952"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  }
 ]
}
